"""
Merges the anomalies, the corruptions words are added and the sentiments averaged. 

"""



import settings
import pandas as pd
import numpy as np
from tqdm import tqdm
tqdm.pandas()



anomaly_information = pd.read_pickle(settings.DATA_NEW+settings.ANOMALY_SVM)
df_daily = pd.read_pickle(settings.DATA_NEW+settings.DAILY_FILE)

cols = list(anomaly_information.columns.values)
anomaly_information.reset_index(inplace=True)
anomaly_information.columns = ['anomaly_id'] + cols

# The old file need this rename function. The file generated by the create_daily should already have the right names
#df_daily = df_daily.rename(columns={'senti_per_agency':'avg_senti_buffer','senti_avg_per_agency':'avg_senti_article'})


df_anomalies_only = pd.DataFrame()

def append_to_df(series):
    """
    Returns the articles during the anomaly period
    """

    start = series['Start Date']
    end = series['End Date']
    agency = series['Agency']

    date_constraint = (df_daily['date'] >= start) & (df_daily['date'] <= end)
    agency_constraint = df_daily['agency'] == agency
    articles_constraint = df_daily['num_articles'] > 0
    all_constraints = date_constraint & articles_constraint & agency_constraint
    
    #Select articles corresponding to an agency
    _tmp = df_daily[all_constraints.values]
    #Indicate anomaly it belongs to
    _tmp['anomaly_id'] = series.anomaly_id
    return _tmp
    

for i in tqdm(range(anomaly_information.shape[0])):
    df_anomalies_only = pd.concat([df_anomalies_only, append_to_df(anomaly_information.iloc[i,:])])
    

def aggregate_lists(df):
    '''
    Averages the sentiment, sum the counts for the articles during an anomaly period
    '''
    df['words_all'] = 0 
    df['words_buffer'] = 0
    df['total_sentences'] = 0
    df['total_buffer_sentence'] = 0
    for i, row in df.iterrows():
        count_all = np.sum([len(sentence.split()) for sentence in row.story])
        count_buffer = np.sum([len(sentence.split()) for sentence in row.story_sentences])
        count_sentences = np.sum([len(sentence) for sentence in row.story_sentence_index])
        count_sentences_buffer = np.sum([len(sentence) for sentence in row.buffered_story_sentence_index])
        df.loc[i,'avg_senti_buffer'] = np.mean(row.avg_senti_buffer)
        df.loc[i,'avg_senti_article'] = np.mean(row.avg_senti_article)
        df.loc[i,'words_all'] = count_all
        df.loc[i,'words_buffer'] = count_buffer
        df.loc[i,'total_sentences'] = count_sentences
        df.loc[i,'total_buffer_sentence'] = count_sentences_buffer
    
    df_anomalies_only['avg_senti_article'] = df_anomalies_only['avg_senti_article'].astype(float)
    df_anomalies_only['avg_senti_buffer'] = df_anomalies_only['avg_senti_buffer'].astype(float)

    
    return df
df_anomalies_only = aggregate_lists(df_anomalies_only)


column_names_sum = list(df_anomalies_only)
to_remove = ('date','agency','newspaper','article_id','title','num_sentences','story','story_sentences','story_sentence_index',
 'buffered_story_sentence_index','avg_senti_buffer','avg_senti_article','v0_corrupt_words_articles_syn','v1_corrupt_words_buffer_syn',
 'v2_corrupt_words_articles_dan','v3_corrupt_words_buffer_dan','anomaly_id')

column_names_sum = [e for e in column_names_sum if e not in to_remove]
values = ['sum']*len(column_names_sum)
columns_agg = dict(zip(column_names_sum, values))
columns_agg.update({'avg_senti_buffer' : 'mean','avg_senti_article':'mean'})

grouped = df_anomalies_only.groupby('anomaly_id').agg(columns_agg)

aggregated_df = anomaly_information.join(grouped,on='anomaly_id')
aggregated_df = aggregated_df.rename(columns={'num_first_page':'total_front_page','num_articles':'total_articles'})

aggregated_df.to_pickle(settings.DATA_NEW+settings.AGGREGATED)
